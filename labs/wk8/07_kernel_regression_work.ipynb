{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pandaxin8/COVID-19/blob/master/labs/wk8/07_kernel_regression_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv8lP8tHjJYT"
      },
      "source": [
        "# Kernel Methods"
      ],
      "id": "Vv8lP8tHjJYT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alrL00TJjJYc"
      },
      "source": [
        "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
      ],
      "id": "alrL00TJjJYc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD_oKJ1sjJYd"
      },
      "source": [
        "In this lab, we will understand the relationship between linear regression and kernel regression by an excercise in predicting the median value of houses in Boston in two ways: using a quadratic basis and using an equivalent kernel function.\n",
        "\n",
        "### Assumed knowledge\n",
        "- Linear regression (week 2)\n",
        "- Kernel methods (week 7 lectures)\n",
        "\n",
        "\n",
        "### Resources\n",
        "- Lecture recording and notes (week 6 lectures)\n",
        "- Bishop, Chapter 6 (Kernel Methods), Introduction and Section 1 (Dual Representations)\n",
        "- Murphy's Machine Learning A Probabilistic Perspective, Chapter 14 (Kernels), 14.2 (Kernel Functions) and 14.4.3 (Kernelised Ridge Regression)\n",
        "\n",
        "\n",
        "### After this lab, you should be comfortable with:\n",
        "- Applying machine learning techniques with a non-linear basis\n",
        "- Using kernel methods instead of a new basis\n",
        "- Evaluating when using a kernel method would or would not be sensible"
      ],
      "id": "OD_oKJ1sjJYd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7iBtFHgjJYf"
      },
      "source": [
        "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
        "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
        "\n",
        "Setting up the environment"
      ],
      "id": "S7iBtFHgjJYf"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fHzXRwIijJYf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "fHzXRwIijJYf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9up1g_REjJYi"
      },
      "source": [
        "## Loading the dataset\n",
        "In this lab, we will use the same [dataset](https://machlearn.gitlab.io/sml2019/tutorials/02-dataset.csv) as in week 2, which describes the price of housing in Boston (see [description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)). \n",
        "We aim to predict the value of a home from other factors.\n",
        "In this dataset, each row represents the data of one house. The first column is the value of the house, which is the target to predict. The remaining columns are features, which has been normalised to be in the range $[-1, 1]$. The corresponding labels of columns are\n",
        "\n",
        "```'medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'```\n",
        "\n",
        "Download the dataset. Read in the data using ```np.loadtxt``` with the optional argument ```delimiter=','```, as our data is comma separated rather than space separated. Remove the column containing the binary variable ```'chas'```.\n",
        "\n",
        "Check that the data is as expected using ```print()```. It should have 506 rows (examples) and 13 columns (1 label and 12 features). Check that this is the case. \n",
        "\n",
        "Hint: use  assert."
      ],
      "id": "9up1g_REjJYi"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yHll08CvjJYj"
      },
      "outputs": [],
      "source": [
        "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']"
      ],
      "id": "yHll08CvjJYj"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xurq6e9R2kaN",
        "outputId": "24954997-4c03-47dd-c44d-16400333940b"
      },
      "id": "Xurq6e9R2kaN",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data = np.loadtxt('/content/drive/MyDrive/SML/labs/wk8/07-dataset.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "kxzD14tV269g"
      },
      "id": "kxzD14tV269g",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RuHgKGDfjJYk",
        "outputId": "81af4194-b521-4973-e6ac-b3f868250c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['medv', 'crim', 'zn', 'indus', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
            "[[24.    -1.    -0.64  -0.864 -0.37   0.155  0.283 -0.462 -1.    -0.584\n",
            "  -0.426  1.    -0.821]\n",
            " [21.    -1.    -1.    -0.515 -0.654  0.096  0.565 -0.302 -0.913 -0.79\n",
            "   0.106  1.    -0.591]\n",
            " [34.    -1.    -1.    -0.515 -0.654  0.389  0.199 -0.302 -0.913 -0.79\n",
            "   0.106  0.979 -0.873]\n",
            " [33.    -0.999 -1.    -0.874 -0.7    0.317 -0.116 -0.103 -0.826 -0.866\n",
            "   0.298  0.989 -0.933]\n",
            " [36.    -0.999 -1.    -0.874 -0.7    0.374  0.057 -0.103 -0.826 -0.866\n",
            "   0.298  1.    -0.801]]\n"
          ]
        }
      ],
      "source": [
        "#loaded_data = np.loadtxt('07-dataset.csv', delimiter=',')\n",
        "\n",
        "# remove chas\n",
        "column_idxes = list(range(len(names)))\n",
        "chas_idx = names.index('chas')\n",
        "wanted_columns = list(column_idxes)\n",
        "wanted_columns.remove(chas_idx)\n",
        "data = loaded_data[:,wanted_columns]\n",
        "data_names = list(names)\n",
        "data_names.remove('chas')\n",
        "\n",
        "print(data_names)\n",
        "print(np.array_str(data[:5], precision=3))\n",
        "assert data.shape == (506,13)"
      ],
      "id": "RuHgKGDfjJYk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIDtSnsQjJYm"
      },
      "source": [
        "## Part 0: Introduction"
      ],
      "id": "EIDtSnsQjJYm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Vzn_nWjJYo"
      },
      "source": [
        "We breifly review some concepts from the class: you may also want to look at the Resources list.\n",
        "\n",
        "### Kernel Functions and Basis Functions\n",
        "Kernel functions can be understood as a way to measure the similarity of two objects $x$ and $x'$ in some space $\\mathcal{X}$,\n",
        "$$\n",
        "k: \\mathcal{X} \\times \\mathcal{X} \\to  \\mathbb{R}\n",
        "$$\n",
        "and especially, we mostly wish to consider positive definite kernels, which we will just call kernels. One powerful property of such kernels is that theses are exactly the functions which can be expressed as the inner product of some basis function transformation:\n",
        "$$\n",
        "k: \\mathcal{X} \\times \\mathcal{X} \\to  \\mathbb{R} \\: \\text{is a kernel}  \\iff k(x,x') = \\langle \\phi_{k}(x), \\phi_{k}(x')  \\rangle_{\\mathcal{V}}\n",
        "$$\n",
        "for some basis function $\\phi_k : \\mathcal{X}\\to \\mathcal{V}$, and inner product $\\langle . \\: , \\: \\rangle_\\mathcal{V}$. For our purposes, $\\mathcal{X}$ and $\\mathcal{V}$ may be simply real spaces $\\mathbb{R}^{d}$, $\\mathbb{R}^{d'}$, and $\\langle x,y\\rangle_\\mathbb{R^{d'}}=x^T y$. \n",
        "\n",
        "The important aspect is the correspondence between some kernel function and a basis function. Fix our standard inner product on real spaces as above. Given a kernel, we can find a basis function whose inner product equals the kernel; given a basis function, we can define a kernel via the inner product:\n",
        "$$\n",
        "k \\to \\phi_k \\: s.t. \\: \\phi_k(x)^T \\phi_k(y) = k(x,y)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\phi \\to k_{\\phi} \\: s.t. \\: k_{\\phi}(x,y) :=  \\phi(x)^T \\phi(y)\n",
        "$$\n",
        "\n",
        "In Part 2 of the lab, you will be given a kernel and required to prove that it is expressible via some basis fucntion.\n",
        "\n",
        "#### Quesion 0.1: While all (positive definite) kernels are expressible as an inner product of basis functions in some space, not all are expressible in finite dimensional space. Give an example of a kernel which cannot be expressed via a finite dimensional basis function."
      ],
      "id": "V5Vzn_nWjJYo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeOsE7XZjJYq"
      },
      "source": [
        "### <span style=\"color:blue\">Answer</span>\n",
        "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
      ],
      "id": "OeOsE7XZjJYq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "My response:\n",
        "------------------\n",
        "\n",
        "\n",
        "Gaussian kernel is an example of a kernel that can't be expressed via a finite dimensional basis function.\n",
        "\n",
        " The Gaussian kernel is defined as:\n",
        "\n",
        "$k(x, y) = exp(-||x - y||^2 / (2œÉ^2))$\n",
        "\n",
        "where x and y are input vectors, ||x - y||^2 is the squared Euclidean distance between them, and œÉ is a positive parameter called the bandwidth.\n",
        "\n",
        "- The Gaussian kernel maps the input data into an infinite-dimensional space, meaning that it cannot be represented by a finite-dimensional basis function\n",
        "- This is because the Gaussian kernel is related to the Fourier transform of a Gaussian function, which results in an infinite series of basis functions\n",
        "- The infinite-dimensional nature of the Gaussian kernel is what allows it to be a powerful and flexible tool for modeling complex and non-linear relationships in data."
      ],
      "metadata": {
        "id": "uPtTIdZZ6uh7"
      },
      "id": "uPtTIdZZ6uh7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av83bMmxjJYq"
      },
      "source": [
        "\n",
        "### Kernel Methods\n",
        "#### Question 0.2: Explain the basic idea of kernel methods to a classmate. Your answer should cover:\n",
        "- In what sense is a kernel method non-parametric?\n",
        "- In contrast to a parametric model like standard linear regression, how does a kernel method use the training data to eventually make a prediction for a new datapoint? Outline the general structure of both a paramatric model and a kernel model.\n",
        "- In the kernel methods context, what is the Gram matrix $\\mathbf{K}$, and what is an entry in it in terms of the kernel function $k$ and dataset $\\{x_i\\}_{n=1}^{N}$?"
      ],
      "id": "Av83bMmxjJYq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4gZohtjJYr"
      },
      "source": [
        "### <span style=\"color:blue\">Answer</span>\n",
        "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
      ],
      "id": "9b4gZohtjJYr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "My response:\n",
        "-----------------------\n",
        "1. Basic idea of kernel methods:\n",
        "\n",
        "Kernel methods are a class of non-parametric machine learning algorithms that operate by mapping input data to a higher-dimensional space, often implicitly, using a kernel function. The kernel function measures the similarity between data points in this higher-dimensional space, allowing the algorithm to find non-linear relationships in the data more easily.\n",
        "\n",
        "2. Non-parametric nature of kernel methods:\n",
        "\n",
        "A kernel method is considered non-parametric because it doesn't assume a specific functional form or a fixed number of parameters for the relationship between inputs and outputs. Instead, it directly relies on the training data and the chosen kernel function to define the model. This flexibility allows kernel methods to adapt to a wide range of data distributions and model complex relationships that might not be well-represented by a parametric model with a fixed functional form.\n",
        "\n",
        "3. Parametric vs. kernel model prediction:\n",
        "\n",
        "In a parametric model like standard linear regression, the relationship between inputs (features) and outputs (labels) is defined by a fixed functional form with a finite number of parameters. For example, in linear regression, the relationship is given by y = wx + b, where w and b are the parameters (weights and bias) to be learned from the training data. To make a prediction for a new data point, the model applies the learned parameters to the input features using the assumed functional form.\n",
        "\n",
        "In contrast, a kernel method uses the training data more directly. For a new data point, the kernel method computes the similarity between the new point and each of the training data points using the kernel function. The prediction is then made by combining the similarities with the training outputs (labels) in a weighted manner. This approach allows kernel methods to capture non-linear relationships without explicitly specifying a functional form.\n",
        "\n",
        "4. Gram matrix and its entries:\n",
        "\n",
        "In the context of kernel methods, the Gram matrix (denoted as ùöø or K) is a matrix that contains the pairwise similarities between all the data points in the dataset, as computed by the kernel function k. The entries of the Gram matrix are defined as:\n",
        "\n",
        "$K(i, j) = k(x_i, x_j)$\n",
        "\n",
        "where $x_i$ and $x_j$ are the i-th and j-th data points in the dataset ${x_i}_{n=1}^{N}$, and k is the kernel function. The Gram matrix plays a crucial role in kernel methods, as it encapsulates the similarity information used to make predictions for new data points."
      ],
      "metadata": {
        "id": "azfXSP576qY0"
      },
      "id": "azfXSP576qY0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3evjvW_4jJYr"
      },
      "source": [
        "### Kernel Methods, Linear Regression and Dual Representation\n",
        "This lab compares a familiar technique, regularised linear regression with a basis function, to the method of kernel regression. \n",
        "\n",
        "We recall that our linear regression optimises the parameter $\\mathbf{w}$ of our model $f_{w}(x')=\\mathbf{w}^T \\phi(x')$ on transformed datapoints $x'$ best minimises the regularised sum-of-squares loss\n",
        "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N(\\mathbf{w}^T \\phi(\\mathbf{x_n}) - t_n)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T\\mathbf{w}, \\:\\:\\:\\:(A)$$\n",
        "Now, ultimately we will perform kernel regression by a function\n",
        "$$\n",
        "f(x') = \\mathbf{k}(x')^T (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{t}\n",
        "$$\n",
        "where $\\mathbf{k}(x')$ is the vector of kernel outputs $k(x', x_i)$ for each $x_i$ in the training data. First however, we wish to derive an expression for the loss as a function of $\\mathbf{a}$ in terms of $\\mathbf{a}$ and the Gram matrix $\\mathbf{K}$:\n",
        "$$J(\\mathbf{a}) = \\frac{1}{2} \\mathbf{a}^T \\mathbf{KKa} - \\mathbf{a}^T \\mathbf{Kt} + \\frac{1}{2}\\mathbf{t}^T \\mathbf{t} + \\frac{\\lambda}{2} \\mathbf{a}^T\\mathbf{Ka} , \\:\\:\\:\\:(B)$$\n",
        "where $\\mathbf{K=\\Phi \\Phi}^T$ with elements $K_{nm} = \\phi(\\mathbf{x_n})^T\\phi(\\mathbf{x_m})$, and $\\mathbf{a}$ is the vector with entries $a_n = -\\frac{1}{\\lambda} (\\mathbf{w}^T \\phi(x_n)-t_n)$ for each $(x_n, t_n)$ in the training data.\n",
        "\n",
        "#### Question 0.3: Derive (B) from (A) (don't spend too long on this, you may leave it and continue with the lab)"
      ],
      "id": "3evjvW_4jJYr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exgh_mi1jJYs"
      },
      "source": [
        "### <span style=\"color:blue\">Answer</span>\n",
        "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>\n",
        "\n",
        "My response:\n",
        "- Come back to this"
      ],
      "id": "Exgh_mi1jJYs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2i7NIzujJYs"
      },
      "source": [
        "## Overview of Lab\n",
        "\n",
        "### Part 0.5: Linear Regression Review\n",
        "We breifly review linear regression with basis functions, and you implement code to find the analytic solution to $\\mathbf{w}$\n",
        "\n",
        "### Part 1: Linear Regression with Quadratic Basis Function\n",
        "Implement a quadratic basis function $\\phi(x)$ and perform linear regression.\n",
        "\n",
        "### Part 2: Linear Regression with a Corresponding Kernel\n",
        "Show that a given kernel function $k(x,y)$ is equivalent to the quadratic basis in Part 1. Implement a calculation of $k(x,y)$ across datasets via matrix operations, and perform kernelised linear regression.\n",
        "\n",
        "### Part 3: Comparison and Reflection\n",
        "Discuss the above linear regression and kernelised linear regression.\n",
        "\n",
        "### Part 3.5: Other Kernels (Optional)\n",
        "Implement a new kernel $k'(x,y)$ of your choice, and see it used in kernelised linear regression.\n",
        "\n"
      ],
      "id": "s2i7NIzujJYs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N173xl5ajJYs"
      },
      "source": [
        "## Part 0.5: Linear Regression Review\n",
        "\n",
        "First remind yourself of what linear regression is by our implemention of linear regression with regularisation on the Boston dataset. Use 80% of the available data for training the model using maximum likelihood with regularisation (assuming Gaussian noise). The rest of the data is allocated to the test set.\n",
        "\n",
        "Report the root mean squared error (RMSE) for the training set and the test set. We'll compare the results with the linear regression with basis function, and the linear regression with kernel function later on.\n",
        "\n",
        "**TODO**: Implement the analytic solution of $\\frac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}}=0$ in the function  ```w_ml_regularised(Phi, t, l)```, where ```l``` stands for $\\lambda$. Note that $$\\Phi=\n",
        "\\begin{pmatrix}\n",
        "    \\phi(\\mathbf{x_1})^T \\\\\n",
        "    \\phi(\\mathbf{x_2})^T \\\\\n",
        "    \\vdots \\\\\n",
        "    \\phi(\\mathbf{x_n})^T \\\\\n",
        "\\end{pmatrix},$$\n",
        "where $\\phi(\\mathbf{x_i})$ denotes the feature vector for the $i$-th datapoint, and $n$ denotes the total number of datapoints."
      ],
      "id": "N173xl5ajJYs"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XVh11BL1jJYt",
        "outputId": "0f76094e-5928-47ee-8a34-643763d09bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression: RMSE with regularisation: Train 4.741045, Test 4.891587\n",
            "Expected value: Train 4.74, Test 4.89\n"
          ]
        }
      ],
      "source": [
        "def w_ml_regularised(Phi, t, l):\n",
        "    \"\"\"Produce the analytic solution of w given input features and labels.\"\"\"\n",
        "    \n",
        "    # Compute the regularized term: lambda * identity matrix\n",
        "    reg_term = l * np.identity(Phi.shape[1])\n",
        "    \n",
        "    # Compute the inverse of (Phi^T * Phi + lambda * I)\n",
        "    inv_term = np.linalg.inv(np.dot(Phi.T, Phi) + reg_term)\n",
        "    \n",
        "    # Compute the solution for w\n",
        "    w = np.dot(inv_term, np.dot(Phi.T, t))\n",
        "    \n",
        "    return w\n",
        "\n",
        "def split_data(data, train_size):\n",
        "    \"\"\"Randomly split data into two groups. The first group is a fifth of the data.\"\"\"\n",
        "    np.random.seed(1)\n",
        "    N = len(data)\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "    train_idx = idx[:int(train_size * N)]\n",
        "    test_idx = idx[int(train_size * N):]\n",
        "\n",
        "    # Assume label is in the first column\n",
        "    X_train = data[train_idx, 1:]\n",
        "    t_train = data[train_idx, 0]\n",
        "    X_test = data[test_idx, 1:]\n",
        "    t_test = data[test_idx, 0]\n",
        "    \n",
        "    return X_train, t_train, X_test, t_test\n",
        "\n",
        "def rmse(X_train, t_train, X_test, t_test, w):\n",
        "    \"\"\"Return the RMSE for training and test sets\"\"\"\n",
        "    N_train = len(X_train)\n",
        "    N_test = len(X_test)\n",
        "\n",
        "    # Training set error\n",
        "    t_train_pred = np.dot(X_train, w)\n",
        "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
        "\n",
        "    # Test set error\n",
        "    t_test_pred = np.dot(X_test, w)\n",
        "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
        "\n",
        "    return rmse_train, rmse_test\n",
        "\n",
        "def lr(X, l, split_rate):\n",
        "    \"\"\"Return RMSE for the training set and the test set\n",
        "    \n",
        "    Parameters\n",
        "    ------------------------------------------------------\n",
        "    X: numpy matrix, whole dataset \n",
        "    l: float, regularisation parameter\n",
        "    split_rate: int, the percent of training dataset\n",
        "    \n",
        "    Returns\n",
        "    ------------------------------------------------------\n",
        "    train_rmse: float, RMSE for training set\n",
        "    test_rmse: float, RMSE for testing set\n",
        "    \"\"\"\n",
        "    X0 = np.ones((data.shape[0], 1))\n",
        "    X = np.hstack((X, X0))\n",
        "\n",
        "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
        "    # alternatively: use train_test_split\n",
        "    #X_train, X_test, t_train, t_test = train_test_split(X[:,1:], X[:, 0], test_size = 1 - split_rate, random_state = 42)\n",
        "    w_reg = w_ml_regularised(X_train, t_train,l)\n",
        "\n",
        "    train_rmse, test_rmse = rmse(X_train, t_train, X_test, t_test, w_reg)\n",
        "    return train_rmse, test_rmse\n",
        "\n",
        "train_rmse, test_rmse = lr(data, 1.1, 0.8)\n",
        "print(\"Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
        "print(\"Expected value: Train 4.74, Test 4.89\")"
      ],
      "id": "XVh11BL1jJYt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS28wDvMjJYu"
      },
      "source": [
        "## Part 1: Linear regression with quadratic basis function\n",
        "\n",
        "Let $X \\in \\RR^{n \\times d}$ be the data matrix containing n datapoints $\\mathbf{x_i} \\in \\RR^{d}$. We can choose to train and test using the raw data $X$ as the input to our model as the above method. Alternatively, we could use $$\\Phi= [\\mathbf{\\phi(x_1)}, \\mathbf{\\phi(x_2)}, ..., \\mathbf{\\phi(x_n)}]^T \\in \\RR^{n \\times m},$$ where $\\mathbf{\\phi(x_i)}$ is some transformation of $\\mathbf{x_i}$, m is the dimension of $\\mathbf{\\phi(x_i)}$.\n",
        "\n",
        "For this lab, write $\\mathbf{x_i} = (x_{i,1},\\dots,x_{i,d})$. Let\n",
        "$$\n",
        "\\phi(\\mathbf{x_i}) = (x_{i,1}^2, x_{i,2}^2, \\ldots, x_{i,d}^2, \\sqrt{2}x_{i,1} x_{i,2}, \\ldots, \\sqrt{2}x_{i,d-1}x_{i,d}, \\sqrt{2}x_{i,1}, \\ldots, \\sqrt{2}x_{i,d}, 1).\n",
        "$$\n",
        "Note that $\\phi(\\mathbf{x_i})$ is all quadratic functions of elements of $\\mathbf{x_i}$ and 1 (The $\\sqrt{2}$ coefficients are for normalisation for later in the lab).\n",
        "We say that we are using a *quadratic basis function*.\n",
        "\n",
        "Train and test the data with ```quadratic_lr```, report the RMSE for the training set and the test set.\n",
        "\n",
        "**TODO**:  \n",
        "1. write a function called ```phi_quadratic``` with a single datapoint $\\mathbf{x_i}$ as input and the quadratic basis function $\\phi(\\mathbf{x_i})$ as output.  \n",
        "2. write a function called ```feature_map``` with raw data matrix $X$ as input and $\\Phi$ as output by using ```phi_quadratic``` for each datapoint. \n",
        "3. in function ```quadratic_lr```, make use of previous functions and give the analytic solution for $\\mathbf{w}$."
      ],
      "id": "oS28wDvMjJYu"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-NDkksPFjJYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de84bb2-e50b-4506-8155-3a684e6a8a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quadratic basis: RMSE with regularisation: Train 2.790632, Test 3.339125\n",
            "Expected value: Train 2.79, Test 3.34\n"
          ]
        }
      ],
      "source": [
        "def phi_quadratic(x):\n",
        "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\n",
        "       \n",
        "       1. Compute quadratic basis function for a single data point 'x' of shape (d,)\n",
        "       2. First computes quadratic terms by squaring each element of 'x'\n",
        "       3. Then computes the interaction terms as pairwise products of elems of 'x', scaled by 'sqrt(2)'\n",
        "       4. Then linear terms are computed by scaling 'x' by 'sqrt(2)'\n",
        "       5. Then constant term is added as array of one element [1]\n",
        "       6. Computed terms are concatenated into a single feature vector 'feat' of shape (m,)\n",
        "       7. Function returns the computed feature vector 'feat'\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    d = len(x)\n",
        "    quadratic_terms = x**2\n",
        "    interaction_terms = np.sqrt(2) * np.array([x[i] * x[j] for i in range(d) for j in range(i+1, d)])\n",
        "    linear_terms = np.sqrt(2) * x\n",
        "    constant_term = np.array([1])\n",
        "    \n",
        "    feat = np.concatenate([quadratic_terms, interaction_terms, linear_terms, constant_term])\n",
        "    return feat # (m,)\n",
        "\n",
        "def feature_map(X):\n",
        "    \"\"\"Return the matrix of the feature map.\n",
        "      X.shape = (n,d)\n",
        "\n",
        "      1. Compute feature map for dataset 'X' of shape (n,d) using quadratic basis fx\n",
        "      2. Apply 'phi_quadratic' fx to each row (data point) of 'X'\n",
        "      3. Resulting matrix 'Phi' has shape (n,m) where m is dimension of transformed feature vector\n",
        "    \"\"\"\n",
        "    \n",
        "    Phi = np.apply_along_axis(phi_quadratic, 1, X)\n",
        "    return Phi # (n,m)\n",
        "\n",
        "def quadratic_lr(X, l, split_rate):\n",
        "    \"\"\"Return RMSE for the training set and the test set\n",
        "    \n",
        "    Parameters\n",
        "    ------------------------------------------------------\n",
        "    X: numpy matrix, whole dataset \n",
        "    l: float, regularisation parameter\n",
        "    split_rate: int, the percent of training dataset\n",
        "    \n",
        "    Returns\n",
        "    ------------------------------------------------------\n",
        "    train_rmse: float, RMSE for training set\n",
        "    test_rmse: float, RMSE for testing set\n",
        "\n",
        "    How it works\n",
        "    1. Performs LR with quadratic basis function on dataset 'X' of shape (n,d)\n",
        "      - regularisation param 'l' and split rate to divide dataset in train and test\n",
        "    2. Splits dataset into train and test\n",
        "    3. Computes feature maps 'Phi_train' and 'Phi_test'\n",
        "    4. Optimal weights are computed using 'w_ml_regularised' (part 0.5)\n",
        "    5. Compute RMSE using feature maps and optimal weights\n",
        "    \"\"\"\n",
        "    \n",
        "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
        "\n",
        "    Phi_train = feature_map(X_train)\n",
        "    w_reg = w_ml_regularised(Phi_train, t_train, l)\n",
        "\n",
        "    train_rmse, test_rmse = rmse(feature_map(X_train), t_train, feature_map(X_test), t_test, w_reg)\n",
        "    return train_rmse, test_rmse\n",
        "\n",
        "train_rmse, test_rmse = quadratic_lr(data, 1.1, 0.8)\n",
        "print(\"Quadratic basis: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
        "print(\"Expected value: Train 2.79, Test 3.34\")"
      ],
      "id": "-NDkksPFjJYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGazTPwujJYv"
      },
      "source": [
        "## Part 2: Linear regression with a  Corresponding Kernel\n",
        "\n",
        "### Computing a basis transformation as an inner product\n",
        "\n",
        "Define $k(\\mathbf{x,y}) = (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2$, where $\\mathbf{x,y} \\in \\mathbb{R}^2$. One way to verify $k(\\mathbf{x,y})$ is a kernel function is to write this as an inner product of a verctor valued function evaluated at $\\mathbf{x}$ and $\\mathbf{y}$. That is, show we have $k(\\mathbf{x}, \\mathbf{y}) = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{y})}$ and specify what is $\\phi(\\mathbf{x}), \\phi(\\mathbf{y})$."
      ],
      "id": "RGazTPwujJYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6W6uMxJjJYw"
      },
      "source": [
        "### <span style=\"color:blue\">Answer</span>\n",
        "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
      ],
      "id": "b6W6uMxJjJYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "My response\n",
        "-------------\n",
        "Given the kernel function $k(\\mathbf{x}, \\mathbf{y}) = (\\langle\\mathbf{x}, \\mathbf{y}\\rangle + 1)^2$, we need to find a feature map $\\phi(\\cdot)$ such that $k(\\mathbf{x}, \\mathbf{y}) = \\langle\\phi(\\mathbf{x}), \\phi(\\mathbf{y})\\rangle$.\n",
        "\n",
        "Let $\\mathbf{x} = (x_1, x_2)$ and $\\mathbf{y} = (y_1, y_2)$, then the kernel function is:"
      ],
      "metadata": {
        "id": "CZi4JAk1MnoV"
      },
      "id": "CZi4JAk1MnoV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKOqJHc0jJYw"
      },
      "source": [
        "Then convince yourself that, for $X \\in \\RR^{n \\times d}, Y \\in \\RR^{m \\times d}$, then $K = (X Y^T  + 1)^2 \\in \\RR^{n \\times m}$ (addition and square term are element-wise) contains elements as kernel function between each pair of datapoints of X and Y,\n",
        "$$K_{ij} = \\phi(\\mathbf{x_i})^T \\phi(\\mathbf{y_j}) = k(\\mathbf{x_i}, \\mathbf{y_j}).$$\n",
        "\n",
        "### Kernelised Regression\n",
        "**TODO**:\n",
        "Write the function ```kernel_quadratic``` which takes $X$, $Y$  as input and $K$ as output."
      ],
      "id": "nKOqJHc0jJYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "My response\n",
        "----------\n",
        "We are given matrices $X \\in \\mathbb{R}^{n \\times d}$ and $Y \\in \\mathbb{R}^{m \\times d}$, and we want to compute the Gram matrix $K = (XY^T + 1)^2 \\in \\mathbb{R}^{n \\times m}$, where the addition and squaring operations are element-wise.\n",
        "\n",
        "Recall that the kernel function is defined as $k(\\mathbf{x}, \\mathbf{y}) = (\\langle\\mathbf{x}, \\mathbf{y}\\rangle + 1)^2$. Our goal is to show that $K_{ij} = k(\\mathbf{x_i}, \\mathbf{y_j}) = \\langle\\phi(\\mathbf{x_i}), \\phi(\\mathbf{y_j})\\rangle$.\n",
        "\n",
        "First, let's compute the matrix product $XY^T$. The element at position $(i, j)$ in the resulting matrix is given by:"
      ],
      "metadata": {
        "id": "9nloPvi5PVic"
      },
      "id": "9nloPvi5PVic"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M5aBEOJ_jJYw"
      },
      "outputs": [],
      "source": [
        "def kernel_quadratic(X, Y):\n",
        "    # Compute the product XY^T\n",
        "    XYT = np.dot(X, Y.T)\n",
        "    \n",
        "    # Add 1 element-wise to the resulting matrix\n",
        "    XYT_plus_1 = XYT + 1\n",
        "    \n",
        "    # Square the elements element-wise\n",
        "    K = np.power(XYT_plus_1, 2)\n",
        "    \n",
        "    return K"
      ],
      "id": "M5aBEOJ_jJYw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtv2sAQajJYx"
      },
      "source": [
        "**TODO**:\n",
        "Complete the function ```kernelised_lr``` with raw data matrix $X$ as input and root mean squared error (RMSE) for the training set and the test set as output. Inside of the function, make use of ```kernel_quadratic``` to apply dual representation, and calculate the predicted labels for training data and testing data repectively."
      ],
      "id": "rtv2sAQajJYx"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IRNH6pUYjJYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb755fe-a315-41b0-afd9-c43e674964e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernelised Regression: RMSE with regularisation: Train 2.790632, Test 3.339125\n",
            "Expected value: Train 2.79, Test 3.33\n"
          ]
        }
      ],
      "source": [
        "def kernelised_lr(X, l, split_rate):\n",
        "    \"\"\"Return RMSE for the training set and the test set\n",
        "    \n",
        "    Parameters\n",
        "    ------------------------------------------------------\n",
        "    X: numpy matrix, whole dataset \n",
        "    l: float, regularisation parameter\n",
        "    split_rate: int, the percent of training dataset\n",
        "    \n",
        "    Returns\n",
        "    ------------------------------------------------------\n",
        "    rmse_train: float, RMSE for training set\n",
        "    rmse_test: float, RMSE for testing set\n",
        "    \"\"\"\n",
        "    \n",
        "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
        "    N_train = X_train.shape[0]\n",
        "    N_test = X_test.shape[0]\n",
        "    \n",
        "    # Compute the kernel matrices for the training and test sets\n",
        "    K_train = kernel_quadratic(X_train, X_train)\n",
        "    K_test = kernel_quadratic(X_test, X_train)\n",
        "    \n",
        "    # Compute the dual solution alpha\n",
        "    alpha = np.linalg.solve(K_train + l * np.eye(N_train), t_train)\n",
        "    \n",
        "    # Training set error\n",
        "    t_train_pred = np.dot(K_train, alpha)\n",
        "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
        "\n",
        "    # Test set error\n",
        "    t_test_pred = np.dot(K_test, alpha)\n",
        "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
        "\n",
        "    return rmse_train, rmse_test\n",
        "\n",
        "train_rmse, test_rmse = kernelised_lr(data, 1.1, 0.8)\n",
        "print(\"Kernelised Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
        "print(\"Expected value: Train 2.79, Test 3.33\")"
      ],
      "id": "IRNH6pUYjJYx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of dual solution alpha**\n",
        "\n",
        "In the context of kernel-based methods, the dual solution, often denoted as $\\boldsymbol{\\alpha}$, is a vector of coefficients that arises when we reformulate the original linear regression problem using the kernel trick."
      ],
      "metadata": {
        "id": "5flu6jiXTz5v"
      },
      "id": "5flu6jiXTz5v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE3dkisljJYy"
      },
      "source": [
        "## Part 3: Comparison and Reflection\n",
        "\n",
        "### Time complexity\n",
        "\n",
        "Compare the above two methods (quadratic basis function, kernel method) in terms of time complexity.\n",
        "\n",
        "In terms of time complexity, is using a kernel method suitable in this case? What are potential advantages of using a kernel method? Disadvantages? \n",
        "\n",
        "### Representation of the Feature Space\n",
        "\n",
        "In this example, the size of the feature space was quadratic in the number of original features of the dataset. Consider the memory cost of representing the transformed data. Give an example of a kernel with a basis function that should not be attempted to be stored in memory."
      ],
      "id": "EE3dkisljJYy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAkP81p8jJYy"
      },
      "source": [
        "### <span style=\"color:blue\">Answer</span>\n",
        "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
      ],
      "id": "kAkP81p8jJYy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "My response\n",
        "---------------\n",
        "1. Time complexity\n",
        "\n",
        "Quadratic basis function:\n",
        "\n",
        "- The feature transformation $\\phi(\\mathbf{x})$ takes $O(d)$ time for each data point, where $d$ is the number of features. This results in $O(nd)$ time complexity for transforming the entire dataset.\n",
        "- Computing the matrix $\\Phi^T\\Phi$ takes $O(nmd)$ time, where $m$ is the dimension of the transformed feature space.\n",
        "- Solving the linear system $(\\Phi^T\\Phi + \\lambda\\mathbf{I})\\mathbf{w} = \\Phi^T\\mathbf{t}$ takes $O(m^3)$ time.\n",
        "- In total, the time complexity of the quadratic basis function method is $O(nd + nmd + m^3)$ -> $O(nmd)$\n",
        "\n",
        "Kernel method:\n",
        "\n",
        "- Computing the kernel matrix $\\mathbf{K}$ takes $O(n^2d)$ time.\n",
        "- Solving the linear system $(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{t}$ takes $O(n^3)$ time.\n",
        "- In total, the time complexity of the kernel method is $O(n^2d + n^3)$ -> $O(n^3)$\n",
        "\n",
        "2. Representation of feature space: Example kernel with a basis fx that should not be stored in memory\n",
        "\n",
        "The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is an example of a kernel with a basis function that should not be attempted to be stored in memory. The RBF kernel is defined as:\n",
        "\n",
        "$$k(x,y) = \\exp(-\\frac{||x-y||^2}{2\\sigma^2})$$\n",
        "\n",
        "where $\\sigma$ is a positive parameter controlling the width of the Gaussian function.\n",
        "\n",
        "The RBF kernel implicitly maps the input data points to an infinite-dimensional feature space, where the basis functions are an infinite set of radial basis functions. Attempting to explicitly represent the transformed data in this infinite-dimensional feature space would require **infinite memory**, which is infeasible.\n",
        "\n",
        "Using the RBF kernel within kernel methods, such as kernelised support vector machines or kernel ridge regression, allows us to operate in the infinite-dimensional feature space implicitly through the **kernel trick** without explicitly storing the transformed data points. This enables capturing complex patterns and relationships in the data while keeping the memory cost manageable.\n",
        "\n",
        "Keywords: infinite memory, kernel trick, kernelised support vector machine, kernel ridge regression\n",
        "\n"
      ],
      "metadata": {
        "id": "ovD57PRZUC_C"
      },
      "id": "ovD57PRZUC_C"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FHq4gcpMTf7s"
      },
      "id": "FHq4gcpMTf7s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA7u2ykejJYy"
      },
      "source": [
        "## Part 3.5: Other Kernels (Optional)"
      ],
      "id": "IA7u2ykejJYy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsWq_gnTjJYy"
      },
      "source": [
        "We have seen how to implement kerenlised regression for the quadratic basis function. There are of course many other kernels, such as \n",
        "- Guassian kernels\n",
        "- Linear kernels\n",
        "- \n",
        "Choose from the above or another\n",
        "\n",
        "Depending on the kernel function you choose, you may find it easier to implement the function itself (k(x,y)) and use it in your implementation of \"your_kernel\", or you may be able to directly compute \"your_kernel(X,Y)\"."
      ],
      "id": "MsWq_gnTjJYy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afHAXUl5jJYz"
      },
      "outputs": [],
      "source": [
        "def your_kernel_function(x, y):\n",
        "    \n",
        "    raise NotImplementedError # TODO, or directly do \"your_kernel\"\n",
        "    \n",
        "    return k"
      ],
      "id": "afHAXUl5jJYz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh1f0_v1jJYz"
      },
      "outputs": [],
      "source": [
        "def your_kernel(X, Y):\n",
        "    \n",
        "    raise NotImplementedError # TODO\n",
        "    \n",
        "    return K"
      ],
      "id": "sh1f0_v1jJYz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ2MsbWrjJYz"
      },
      "source": [
        "Now for you to reimplement kernelised linear regression: your code here should not differ significantly from that which you used in Part 2."
      ],
      "id": "kZ2MsbWrjJYz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7__LYo7OjJY0"
      },
      "outputs": [],
      "source": [
        "def your_kernelised_lr(X, l, split_rate):\n",
        "    \"\"\"Return RMSE for the training set and the test set\n",
        "    \n",
        "    Parameters\n",
        "    ------------------------------------------------------\n",
        "    X: numpy matrix, whole dataset \n",
        "    l: float, regularisation parameter\n",
        "    split_rate: int, the percent of training dataset\n",
        "    \n",
        "    Returns\n",
        "    ------------------------------------------------------\n",
        "    rmse_train: float, RMSE for training set\n",
        "    rmse_test: float, RMSE for testing set\n",
        "    \"\"\"\n",
        "    \n",
        "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
        "    N_train = X_train.shape[0]\n",
        "    N_test = X_test.shape[0]\n",
        "    \n",
        "    raise NotImplementedError # TODO\n",
        "    \n",
        "    # Training set error\n",
        "    t_train_pred = None # TODO\n",
        "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
        "\n",
        "    # Test set error\n",
        "    t_test_pred = None # TODO\n",
        "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
        "\n",
        "    return rmse_train, rmse_test\n",
        "\n",
        "train_rmse, test_rmse = your_kernelised_lr(data, 1.1, 0.8)\n",
        "print(\"Kernelised Regression with your Kernel Function: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))"
      ],
      "id": "7__LYo7OjJY0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DjhsI72jJY0"
      },
      "source": [
        "Why did you choose the kernel function that you did? How did it perform relative to the quadratic kernel? In what situation (type of dataset, properties of the data or distribution) might your chosen kernel function be advantagous compared to the quadratic kernel? "
      ],
      "id": "8DjhsI72jJY0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddGCJfrKjJY0"
      },
      "source": [
        "## Textbook Questions (Optional)\n",
        "These questions are hand picked to both be of reasonable difficulty and demonstrate what you are expected to be able to solve. The questions are labelled in Bishop as either $\\star$, $\\star\\star$, or $\\star\\star\\star$ to rate its difficulty.\n",
        "\n",
        "- **Question 6.1**: Understand dual formulation (Difficulty $\\star\\star$, simple algebraic derivation)\n",
        "- **Question 6.2**: Dual formulation for perceptron learning algorithm (Difficulty $\\star\\star$, simple algebraic derivation)\n",
        "- **Question 6.11**: You may want to use Taylor expansion to represent term $\\exp(\\mathbf{x}^T\\mathbf{x}'/2\\sigma^2)$ (Difficulty $\\star$)\n",
        "- **Question 6.12**: To prove $k\\left(A_{1}, A_{2}\\right)=2^{\\left|A_{1} \\cap A_{2}\\right|}=\\phi\\left(A_{1}\\right)^{T} \\phi\\left(A_{2}\\right)$. (Difficulty $\\star\\star$)\n",
        "- **Question 6.13**: Use chain rule to represent $g(\\varphi(\\theta), x)$ (Difficulty $\\star$)\n",
        "- **Question 7.6**: (Difficulty $\\star$, simple algebraic derivation)\n",
        "- **Question 7.7**: (Difficulty $\\star$, simple algebraic derivation)"
      ],
      "id": "ddGCJfrKjJY0"
    }
  ]
}